# H3BERTa Training

**W&B tracking:** [Project link](https://wandb.ai/ibmm-unibe-ch/DALM_CDRH3_PIPELINE3?nw=nwuserchrode)


**SUB-PIPELINE1: IgG_IgA_Bsources_config3_lr5e-5_bs1024 â†’ BEST MODEL TO USE**  

SUB-PIPELINE1 (IgG + IgA, all B-cell sources) achieved the best overall performance among all tested configurations.
- **Best epoch:** 113  
- **Model path:** `./model/SUB-PIPELINE1:IgG_IgA_Bsources/config3.json_lr5e-5_bs1024/BEST_MODEL/epoch_113`  
- **Dataset size:** ~18 million sequences (17,967,980 total for train/validation/test)

